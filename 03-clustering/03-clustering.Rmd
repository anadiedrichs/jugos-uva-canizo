---
title: "Clustering"
author: "Ana Laura Diedrichs"
date: "24 de julio 2020"
output:
  github_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE, echo=FALSE}
library(DataExplorer)
library(factoextra)
library(cluster)
library(fpc)

```


```{r,message=FALSE}

source("../loadData.R")
dataset <- load_dataset()
data <- dataset[-1]

```
# Clustering intro

Vamos a aplicar un enfoque no supervisado sobre los datos mediante agrupamiento, sin considerar la etiqueta Origen o variable de clase. Sí usaremos la misma para relacionar y analizar el agrupamiento obtenido.


Primero usaremos k-means. Previo al uso de este algoritmo, los datos no deben tener valores NULOS o perdidos y son escalados entre 0 y 1.

En el siguiente gráfico muestra como la suma del cuadrado de las distancias intra-cluster disminuye a medida que se agrega un cluster (aumenta k) en kmeans. Computamos el WSS para distintos números de clusteres para k-means para entender como disminuye el total within-cluster sum of square (WSS) a medida que se incrementan los clústeres.

```{r}
# Determine number of clusters
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(data, 
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

Observamos que de un k=1 a un k=2 disminuye en un tercio el WSS. 

## K-means

Nuestra pregunta es, ¿se agrupan "naturalmente" las muestras por su origen? ¿hay outliers? ¿Hay muestras de Brasil que son parecidas a las de Argentina, viceversa?

Aplicamos k-means clustering considerando $k=2$, ya que tenemos sólo dos etiquetas AR y BR. 

```{r}
set.seed(11235)
# K-Means Cluster Analysis
fit <- kmeans(data, centers=2,nstart = 50) # 2 cluster solution
# append cluster assignment
mydata <- data.frame(data, fit$cluster)
fit
#cbind(mydata$fit.cluster,dataset$Origen)
```

Valores medios de cada variable en cada cluster

```{r}
# get cluster means 
aggregate(data,by=list(fit$cluster),FUN=mean)

```

En la siguiente tabla observamos la distribución de observaciones entre los dos clústeres, según el origen
```{r}
table(mydata$fit.cluster,dataset$Origen)
```

Observamos que para el clúster "2" han sido asignadas la mayoría de las muestras de Argentina, unas `r table(mydata$fit.cluster,dataset$Origen)["2","AR"]` en total y tan solo `r table(mydata$fit.cluster,dataset$Origen)["2","BR"]` de Brasil.


### Análisis o visualización de clústers, k=2

Comparamos en la siguiente tabla la agrupación de elementos en sus clúster 1 o 2 vs su etiqueta Origen

```{r}
table(mydata$fit.cluster,dataset$Origen)
```

Observamos que en el clúster 2 la mayoría de las observaciones son de Argentina, excepto 1 es de Brasil. Considerar esto al analizar los histogramas.

A continuación mostramos la distribución (histograma) de las muestras que fueron agrupadas para el clúster 1 y para el 2.

```{r}
d <- mydata
d$fit.cluster <- as.factor(d$fit.cluster)
plot_boxplot(d,by="fit.cluster")
```

De acuerdo al diagrama de cajas de arriba observamos como k-means agrupó los dos clústeres. El clúster 2, por ejemplo, tiene más concentraciones en promedio de Y, Pd, Fe, Mo. Recordemos que el clúster 2 la mayoría de las observaciones o muestras son de Argentina. **¿Qué características podemos extraer de esto?**

Ahora observaremos la distribución dentro de cada clúster y según su origen.


*Cluster 1*

```{r }
k.1 <- dataset[fit$cluster==1,]
k.2 <- dataset[fit$cluster==2,]
```

```{r }
plot_boxplot(k.1, by = "Origen")
```

*Cluster 2*

```{r }
plot_boxplot(k.2, by = "Origen")
```

Para graficar las observaciones agrupadas en los clusteres, dado que tenemos más de dos dimensiones, se nos complica graficar mas bien visualizar en dos ejes. Usamos la libreria cluster que reduce las dimensiones y permite graficar los clústeres.

```{r}

# vary parameters for most readable graph
clusplot(data, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)

# Centroid Plot against 1st 2 discriminant functions

plotcluster(data, fit$cluster)
```





Comparamos estadisticas de cluster con k=2

```{r}
library(fpc)
distancia <- dist(data)
cluster.stats(distancia, fit$cluster)
```


## Density-based spatial clustering of applications with noise (DBSCAN ) 

El comportamiento de DBSCAN es muy sensible al cambio en los valores eps (radio desde el centro) y MinPts (cantidad de puntos mínima del cluster)

```{r}
library("fpc")
#Notar que es muy diferente el resultado a medida que modificamos e parametro eps y MinPts.
set.seed(123)
db <- fpc::dbscan(data, eps = 0.65, MinPts = 5)
fviz_cluster(db,data)


```
Las observaciones o muestras 1,3,4,7,10, por ejemplo, las toma como outliers o valores extremos. Notamos que hay varios de ellos para la configuración eps=0.65 y minPts=5. 

En la siguiente tabla se muestra como 0 las observaciones que consideró outliers vs su clasificación de origen inicial.

```{r}
table(db$cluster,dataset$Origen)
```

Las observaciones etiquetadas como cero (0) son los outliers.


## Conclusiones

* Se observan algunas variables que pueden ser buenas para clasificar en origen, dado este dataset como Cr.
* Son pocas muestras (26) por lo que no nos animamos a realizar conclusiones globales
* Se observa en ambos algoritmos que al menos uno de los grupos contiene solo observaciones de Argentina
* Tanto DBSCAN como kmeans se ven afectados por la variabilidad de las mediciones / valores extremos. En k-means el cluster 2 incluyó las observaciones que tuvieron más concentración de determinados elementos.
* Ninguno de los grupos es fácilmente identificable, excepto la observación peculiar del caso k-means k=2 donde agrupó la mayoría de las observaciones de Argentina en uno de los clusters.

